"""
Aidan Chang
Class: CS 677
Date: 08/04/2022
Homework Problem #4
Description of Problem: Use Naive Bayesian NB classifier
"""
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier

if __name__ == "__main__":
    #Reading xls into df with sheet_name 
    df = pd.read_excel('CTG.xls', sheet_name='Raw Data')

    #Remove first empty row
    df = df.dropna()
    df = df.iloc[1:]
    df['true_label'] = np.where(df.NSP==1, 1, 0)

    #Group 3 fixed features
    group3_features = ['MSTV', 'Width', 'Mode', 'Variance', 'true_label']
    df= df.astype({"MSTV": "float", "Width": "float", "Mode": "float", "Variance": "float", 'true_label': 'int'})
    df= df[group3_features]
   
    #create dfs that NSP = or != 1 with group3 features only
    df_normal = df.loc[df['true_label']==1, group3_features]
    df_abnormal = df.loc[df['true_label']!=1, group3_features]
    x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :4], df.loc[:, ['true_label']], test_size = 0.5, random_state = 3)

    """
    Problem 4.1
    """
    #n_estimators
    N = 10
    d=5

    plt_df = pd.DataFrame()

    for n in range(N):   
        d_list = []

        for depth in range(d):
            rfc_model = RandomForestClassifier(criterion = 'entropy', max_depth=d+1, n_estimators=n+1)
            rfc_model.fit(x_train, y_train.values.ravel())
            y_pred = rfc_model.predict(x_test)
            TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()
            accuracy = TP/(TP+TN+FP+FN)
            error_rate = 1 - accuracy
            d_list.append(error_rate)
        
        plt_df[n+1] = d_list
        d_list.clear()
    #print(plt_df)

    # """
    # Problem 4.2
    # """
    plt.plot(plt_df.columns,plt_df.loc[0])
    plt.plot(plt_df.columns,plt_df.loc[1])
    plt.plot(plt_df.columns,plt_df.loc[2])
    plt.plot(plt_df.columns,plt_df.loc[3])
    plt.plot(plt_df.columns,plt_df.loc[4])

    plt.title('Random Forest Classifier Error Plot')
    plt.xlabel('Number of Trees')
    plt.ylabel('Error Rate')
    plt.legend(['Depth1', 'Depth2', 'Depth3', 'Depth4', 'Depth5'])
    plt.show()
    
    # """   
    # Problem 4.3
    # """
    # Best accuracy happened at N = 10, d = 1


    # """   
    # Problem 4.4
    # """
    rfc_model = RandomForestClassifier(criterion='entropy', max_depth=4, n_estimators=10)
    rfc_model.fit(x_train, y_train.values.ravel())
    y_pred = rfc_model.predict(x_test)
    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()
    
    print(f'Accuracy for N= 10, d= 4: {TP/(TP+TN+FP+FN)}')
    print(f'Decision Tree Confusion matrix\nTP:{TP}, TN:{TN}, FP:{FP}, FN:{FN}')
    print(f'TPR: {TP/(TP+FN)}')
    print(f'TNR: {TN/(TN+FP)}')